{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification fine-tuning using Helical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell type classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 14:15:43.711747: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 14:15:43.720599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-23 14:15:43.730389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-23 14:15:43.733542: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-23 14:15:43.741604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 14:15:44.300948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/matthew/helical-dev/helical/helical/models/scgpt/model_dir/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n"
     ]
    }
   ],
   "source": [
    "from helical.models.geneformer.geneformer_config import GeneformerConfig\n",
    "from helical.models.geneformer.fine_tuning_model import GeneformerFineTuningModel\n",
    "from helical.models.geneformer.model import Geneformer\n",
    "from helical.models.scgpt.fine_tuning_model import scGPTFineTuningModel\n",
    "from helical.models.scgpt.model import scGPT,scGPTConfig\n",
    "from helical.models.uce.model import UCE, UCEConfig\n",
    "from helical.models.uce.fine_tuning_model import UCEFineTuningModel\n",
    "from helical.utils.dataset_to_anndata import get_anndata_from_hf_dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b538a4b4354c22a129baf8787e7a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c92f39b7a2f4ef8bf1f5a1f266b67c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"helical-ai/yolksac_human\",trust_remote_code=True, download_mode=\"reuse_cache_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/miniconda3/envs/helical-dev/lib/python3.11/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/matthew/miniconda3/envs/helical-dev/lib/python3.11/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_anndata_from_hf_dataset(ds[\"train\"])[:10]\n",
    "test_dataset = get_anndata_from_hf_dataset(ds[\"test\"])[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this classification task we want to predict cell type classes\n",
    "- So we save the cell types as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types_train = list(np.array(train_dataset.obs[\"LVL1\"].tolist()))[:10]\n",
    "cell_types_test = list(np.array(test_dataset.obs[\"LVL1\"].tolist()))[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We convert these string labels into unique integer classes for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set(cell_types_train)\n",
    "class_id_dict = dict(zip(label_set, [i for i in range(len(label_set))]))\n",
    "\n",
    "for i in range(len(cell_types_train)):\n",
    "    cell_types_train[i] = class_id_dict[cell_types_train[i]]\n",
    "\n",
    "for i in range(len(cell_types_test)):\n",
    "    cell_types_test[i] = class_id_dict[cell_types_test[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the desired pretrained Geneformer model and desired configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/geneformer/v1/gene_median_dictionary.pkl' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/geneformer/v1/gene_median_dictionary.pkl'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/geneformer/v1/token_dictionary.pkl' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/geneformer/v1/token_dictionary.pkl'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/geneformer/v1/ensembl_mapping_dict.pkl' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/geneformer/v1/ensembl_mapping_dict.pkl'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/geneformer/v1/gf-6L-30M-i2048/config.json' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/geneformer/v1/gf-6L-30M-i2048/config.json'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/geneformer/v1/gf-6L-30M-i2048/training_args.bin' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/geneformer/v1/gf-6L-30M-i2048/training_args.bin'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/geneformer/v1/gf-6L-30M-i2048/pytorch_model.bin' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/geneformer/v1/gf-6L-30M-i2048/pytorch_model.bin'\n",
      "INFO:helical.models.geneformer.model:Model finished initializing.\n"
     ]
    }
   ],
   "source": [
    "geneformer_config = GeneformerConfig(device=device, batch_size=5, model_name=\"gf-6L-30M-i2048\")\n",
    "geneformer = Geneformer(configurer = geneformer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data so it is in the correct form for Geneformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /home/matthew/.cache/pyensembl/GRCh38/ensembl110/Homo_sapiens.GRCh38.cdna.all.fa.gz.pickle\n",
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /home/matthew/.cache/pyensembl/GRCh38/ensembl110/Homo_sapiens.GRCh38.ncrna.fa.gz.pickle\n",
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /home/matthew/.cache/pyensembl/GRCh38/ensembl110/Homo_sapiens.GRCh38.pep.all.fa.gz.pickle\n",
      "INFO:helical.services.mapping:Mapped 21359 genes to Ensembl IDs from a total of 37318 genes.\n",
      "... storing 'LVL1' as categorical\n",
      "... storing 'LVL2' as categorical\n",
      "... storing 'LVL3' as categorical\n",
      "... storing 'ensembl_id' as categorical\n",
      "100%|██████████| 50/50 [00:31<00:00,  1.60it/s]\n",
      "INFO:helical.models.geneformer.geneformer_tokenizer:/tmp/tmpyjug5tfe.h5ad has no column attribute 'filter_pass'; tokenizing all cells.\n",
      "INFO:helical.models.geneformer.geneformer_tokenizer:Creating dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0595f0eda6764aa98b3272512a43b1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /home/matthew/.cache/pyensembl/GRCh38/ensembl110/Homo_sapiens.GRCh38.cdna.all.fa.gz.pickle\n",
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /home/matthew/.cache/pyensembl/GRCh38/ensembl110/Homo_sapiens.GRCh38.ncrna.fa.gz.pickle\n",
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /home/matthew/.cache/pyensembl/GRCh38/ensembl110/Homo_sapiens.GRCh38.pep.all.fa.gz.pickle\n",
      "INFO:helical.services.mapping:Mapped 21359 genes to Ensembl IDs from a total of 37318 genes.\n",
      "... storing 'LVL1' as categorical\n",
      "... storing 'LVL2' as categorical\n",
      "... storing 'LVL3' as categorical\n",
      "... storing 'ensembl_id' as categorical\n",
      "100%|██████████| 13/13 [00:08<00:00,  1.55it/s]\n",
      "INFO:helical.models.geneformer.geneformer_tokenizer:/tmp/tmpkyxawh14.h5ad has no column attribute 'filter_pass'; tokenizing all cells.\n",
      "INFO:helical.models.geneformer.geneformer_tokenizer:Creating dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323e357c856b4c5082eeee77b1569142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geneformer_train_dataset = geneformer.process_data(train_dataset)\n",
    "geneformer_test_dataset = geneformer.process_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geneformer makes use of the Hugging Face dataset class and so we need to add the labels as a column to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_train_dataset = geneformer_train_dataset.add_column(\"LVL1\", cell_types_train)\n",
    "geneformer_test_dataset = geneformer_test_dataset.add_column(\"LVL1\", cell_types_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Geneformer Fine-Tuning Model from the Helical package which appends a fine-tuning head automatically from the list of available heads\n",
    "- Define the task type, which in this case is classification\n",
    "- Defined the output size, which is the number of unique labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_fine_tune = GeneformerFineTuningModel(geneformer_model=geneformer, fine_tuning_head=\"classification\", output_size=len(label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.geneformer.fine_tuning_model:Freezing the first 2 encoder layers of the Geneformer model during fine-tuning.\n",
      "INFO:helical.models.geneformer.fine_tuning_model:Starting Fine-Tuning\n",
      "Fine-Tuning: epoch 1/1: 100%|██████████| 5069/5069 [06:17<00:00, 13.43it/s, loss=0.06]  \n",
      "Fine-Tuning Validation: 100%|██████████| 1268/1268 [00:44<00:00, 28.42it/s, accuracy=0.989]\n",
      "INFO:helical.models.geneformer.fine_tuning_model:Fine-Tuning Complete. Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "geneformer_fine_tune.train(train_dataset=geneformer_train_dataset, validation_dataset=geneformer_test_dataset, label=\"LVL1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scGPT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same procedure with scGPT\n",
    "- Loading the model and setting desired configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/scgpt/scGPT_CP/vocab.json' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/scgpt/scGPT_CP/vocab.json'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/scgpt/scGPT_CP/best_model.pt' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/scgpt/scGPT_CP/best_model.pt'\n",
      "INFO:helical.models.scgpt.model:Model finished initializing.\n"
     ]
    }
   ],
   "source": [
    "scgpt_config=scGPTConfig(batch_size=10, device=device)\n",
    "scgpt = scGPT(configurer=scgpt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly different methodology for getting the dataset for scGPT since it does not make use of the Hugging Face Dataset class\n",
    "- Split the data into a train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.scgpt.model:Filtering out 10801 genes to a total of 26517 genes with an id in the scGPT vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.scgpt.model:Filtering out 10801 genes to a total of 26517 genes with an id in the scGPT vocabulary.\n"
     ]
    }
   ],
   "source": [
    "dataset = scgpt.process_data(train_dataset)\n",
    "validation_dataset = scgpt.process_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the scGPT fine-tuning model with the desired head and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scgpt_fine_tune = scGPTFineTuningModel(scGPT_model=scgpt, fine_tuning_head=\"classification\", output_size=len(label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scGPT fine tuning we have to pass in the labels as a separate list\n",
    "- This is the same for the validation and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.scgpt.fine_tuning_model:Starting Fine-Tuning\n",
      "Fine-Tuning: epoch 1/1:   0%|          | 1/2535 [00:00<04:20,  9.74it/s, loss=1.68]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning: epoch 1/1: 100%|██████████| 2535/2535 [01:59<00:00, 21.17it/s, loss=0.187]\n",
      "Fine-Tuning Validation: 100%|██████████| 634/634 [00:10<00:00, 60.23it/s, accuracy=0.989]\n",
      "INFO:helical.models.scgpt.fine_tuning_model:Fine-Tuning Complete. Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "scgpt_fine_tune.train(train_input_data=dataset, train_labels=cell_types_train, validation_input_data=validation_dataset, validation_labels=cell_types_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCE Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/uce/all_tokens.torch' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/uce/all_tokens.torch'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/uce/4layer_model.torch' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/uce/4layer_model.torch'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/uce/species_chrom.csv' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/uce/species_chrom.csv'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/uce/species_offsets.pkl' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/uce/species_offsets.pkl'\n",
      "INFO:helical.services.downloader:File: '/home/matthew/.cache/helical/models/uce/protein_embeddings/Homo_sapiens.GRCh38.gene_symbol_to_embedding_ESM2.pt' exists already. File is not overwritten and nothing is downloaded.\n",
      "INFO:helical.services.downloader:File saved to: '/home/matthew/.cache/helical/models/uce/protein_embeddings/Homo_sapiens.GRCh38.gene_symbol_to_embedding_ESM2.pt'\n",
      "/home/matthew/miniconda3/envs/helical-dev/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "INFO:helical.models.uce.model:Model finished initializing.\n"
     ]
    }
   ],
   "source": [
    "uce_config=UCEConfig(batch_size=5, device=device)\n",
    "uce = UCE(configurer=uce_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data the same way as for scGPT\n",
    "- Add names for each dataset, as datasets are stored as .npz files and separate files are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.uce.gene_embeddings:Finished loading gene embeddings for {'human'} from /home/matthew/.cache/helical/models/uce/protein_embeddings\n",
      "INFO:helical.models.uce.gene_embeddings:Filtered out 19355 genes to a total of 17963 genes with embeddings.\n",
      "INFO:helical.models.uce.uce_utils:Passed the gene expressions (with shape=(25344, 37318) and max gene count data 31635.0) to ./train_counts.npz\n",
      "INFO:helical.models.uce.model:Successfully prepared the UCE Dataset.\n",
      "INFO:helical.models.uce.gene_embeddings:Finished loading gene embeddings for {'human'} from /home/matthew/.cache/helical/models/uce/protein_embeddings\n",
      "INFO:helical.models.uce.gene_embeddings:Filtered out 19355 genes to a total of 17963 genes with embeddings.\n",
      "INFO:helical.models.uce.uce_utils:Passed the gene expressions (with shape=(6336, 37318) and max gene count data 20639.0) to ./validation_counts.npz\n",
      "INFO:helical.models.uce.model:Successfully prepared the UCE Dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset = uce.process_data(train_dataset, name=\"train\")\n",
    "validation_dataset = uce.process_data(test_dataset, name=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uce_fine_tune = UCEFineTuningModel(uce_model=uce, fine_tuning_head=\"classification\", output_size=len(label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.uce.fine_tuning_model:Starting Fine-Tuning\n",
      "Fine-Tuning: epoch 1/1: 100%|██████████| 5069/5069 [12:54<00:00,  6.55it/s, loss=1.13]\n",
      "Fine-Tuning Validation: 100%|██████████| 1268/1268 [01:16<00:00, 16.49it/s, accuracy=0.473]\n",
      "INFO:helical.models.uce.fine_tuning_model:Fine-Tuning Complete. Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "uce_fine_tune.train(train_input_data=dataset, train_labels=cell_types_train, validation_input_data=validation_dataset, validation_labels=cell_types_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
