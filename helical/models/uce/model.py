import logging
from typing import Optional
from helical.models.uce.fine_tuning_model import UCEFineTuningModel
import numpy as np
from anndata import AnnData
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader
import scipy
from pathlib import Path
import scanpy as sc
from tqdm import tqdm
import torch
from torch import optim
from torch.nn.modules import loss
from accelerate import Accelerator
from transformers import get_scheduler
from helical.models.uce.uce_config import UCEConfig
from helical.models.base_models import HelicalRNAModel
from helical.models.uce.uce_utils import get_ESM2_embeddings, get_positions, get_protein_embeddings_idxs, load_model, prepare_expression_counts_file
from helical.services.downloader import Downloader
from helical.models.uce.uce_dataset import UCEDataset
from helical.models.uce.gene_embeddings import load_gene_embeddings_adata

LOGGER = logging.getLogger(__name__)
class UCE(HelicalRNAModel):
    """Universal Cell Embedding Model. This model reads in single-cell RNA-seq data and outputs gene embeddings. 
        This model particularly uses protein-embeddings generated by ESM2. 
        Currently we support human and macaque species but you can add your own species by providing the protein embeddings.

        Example
        -------
        >>> from helical.models import UCE, UCEConfig
        >>> import anndata as ad
        >>> configurer=UCEConfig(batch_size=10)
        >>> uce = UCE(configurer=configurer)
        >>> ann_data = ad.read_h5ad("./10k_pbmcs_proc.h5ad")
        >>> dataset = uce.process_data(ann_data[:100])
        >>> embeddings = uce.get_embeddings(dataset)

        Parameters
        ----------
        configurer : UCEConfig, optional, default = default_configurer
            The model configuration.

        Returns
        -------
        None

        Notes
        -----
        The Universal Cell Embedding Papers has been published on `BioRxiv <https://www.biorxiv.org/content/10.1101/2023.11.28.568918v1>`_ and it is built on top of `SATURN <https://www.nature.com/articles/s41592-024-02191-z>`_ published in Nature.
        """
    default_configurer = UCEConfig()

    def __init__(self, configurer: UCEConfig = default_configurer) -> None:    
        super().__init__()
        self.config = configurer.config

        downloader = Downloader()
        for file in self.config["list_of_files_to_download"]:
            downloader.download_via_name(file)

        self.model_dir = self.config['model_path'].parent
        self.device = self.config["device"]
        self.embeddings = get_ESM2_embeddings(self.config["token_file_path"], self.config["token_dim"])
        self.model =  load_model(self.config['model_path'], self.config, self.embeddings)
        self.model = self.model.eval().to(self.device)

        if self.config["accelerator"] or self.device=='cuda':
            self.accelerator = Accelerator(project_dir=self.model_dir)#, cpu=self.config["accelerator"]["cpu"])
            self.model = self.accelerator.prepare(self.model)
        else:
            self.accelerator = None
        LOGGER.info(f"Model finished initializing.")

    def process_data(self, 
                     adata: AnnData, 
                     gene_names: str = "index",
                     name = "test",
                     filter_genes_min_cell: int = None
                     ) -> UCEDataset:
        """Processes the data for the Universal Cell Embedding model

        Parameters 
        ----------
        adata : AnnData
            The AnnData object containing the data to be processed. 
            The UCE model requires the gene expression data as input and the gene symbols as variable names (i.e. as adata.var_names).
        gene_names: str, optional, default = "index"
            The name of the column in the AnnData object that contains the gene symbols.
            By default, the index of the AnnData object is used.
            If another column is specified, that column will be set as the index of the AnnData object.
        name: str, optional, default = "test"
            The name of the dataset. Needed for when slicing AnnData objects for train and validation datasets.
        filter_genes_min_cell: int, default = None
            Filter threshold that defines how many times a gene should occur in all the cells.

        Returns
        -------
        UCEDataset
            Inherits from Dataset class.
        """
                
        self.ensure_rna_data_validity(adata, gene_names)

        if gene_names != "index":
            adata.var.index = adata.var[gene_names]

        files_config = {
            "spec_chrom_csv_path": self.model_dir / "species_chrom.csv",
            "protein_embeddings_dir": self.model_dir / "protein_embeddings/",
            "offset_pkl_path": self.model_dir / "species_offsets.pkl"
        }

        if filter_genes_min_cell is not None:
            sc.pp.filter_genes(adata, min_cells=filter_genes_min_cell)
            # sc.pp.filter_cells(ad, min_genes=25)
        ##Filtering out the Expression Data That we do not have in the protein embeddings
        filtered_adata, species_to_all_gene_symbols = load_gene_embeddings_adata(adata=adata,
                                                                        species=[self.config["species"]],
                                                                        embedding_model=self.config["gene_embedding_model"],
                                                                        embeddings_path=Path(files_config["protein_embeddings_dir"]))
        # TODO: What about hv_genes? See orig.
        gene_expression = adata.X.toarray()

        name = name
        gene_expression_folder_path = "./"
        prepare_expression_counts_file(gene_expression, name, gene_expression_folder_path)
        
        # shapes dictionary
        num_cells = filtered_adata.X.shape[0]
        num_genes = filtered_adata.X.shape[1]
        shapes_dict = {name: (num_cells, num_genes)}

        pe_row_idxs = get_protein_embeddings_idxs(files_config["offset_pkl_path"], self.config["species"], species_to_all_gene_symbols, filtered_adata)
        dataset_chroms, dataset_start = get_positions(Path(files_config["spec_chrom_csv_path"]), self.config["species"], filtered_adata)

        if not (len(dataset_chroms) == len(dataset_start) == num_genes == pe_row_idxs.shape[0]): 
            LOGGER.error(f'Invalid input dimensions for the UCEDataset! ' 
                        f'dataset_chroms: {len(dataset_chroms)}, '
                        f'dataset_start: {len(dataset_start)}, '
                        f'num_genes: {num_genes}, '
                        f'pe_row_idxs.shape[0]: {pe_row_idxs.shape[0]}')
            raise AssertionError
        
        dataset = UCEDataset(sorted_dataset_names = [name],
                             shapes_dict = shapes_dict,
                             model_config = self.config,
                             expression_counts_path = gene_expression_folder_path,
                             dataset_to_protein_embeddings = pe_row_idxs,
                             datasets_to_chroms = dataset_chroms,
                             datasets_to_starts = dataset_start
                             ) 
        LOGGER.info(f'Successfully prepared the UCE Dataset.')
        return dataset

    def get_embeddings(self, dataset: UCEDataset) -> np.array:
        """Gets the gene embeddings from the UCE model

        Parameters
        ----------
        dataset : UCEDataSet
            The Dataset object containing the processed data

        Returns
        -------
        np.array
            The gene embeddings in the form of a numpy array
        """
     
        batch_size = self.config["batch_size"]
        dataloader = DataLoader(dataset, 
                                batch_size=batch_size, 
                                shuffle=False,
                                collate_fn=dataset.collator_fn,
                                num_workers=0)
        

        if self.accelerator is not None:
            dataloader = self.accelerator.prepare(dataloader)


        # disable progress bar if not the main process
        if self.accelerator is not None:
            pbar = tqdm(dataloader, disable=not self.accelerator.is_local_main_process)
        else:
            pbar = tqdm(dataloader)
        
        LOGGER.info(f"Inference started")
        dataset_embeds = []
        
        # disabling gradient calculation for inference
        with torch.no_grad():
            for batch in pbar:
                batch_sentences, mask, idxs = batch[0], batch[1], batch[2]
                batch_sentences = batch_sentences.permute(1, 0)
                if self.config["multi_gpu"]:
                    batch_sentences = self.model.module.pe_embedding(batch_sentences.long())
                else:
                    batch_sentences = self.model.pe_embedding(batch_sentences.long())
                batch_sentences = torch.nn.functional.normalize(batch_sentences, dim=2)  # normalize token outputs
                _, embedding = self.model.forward(batch_sentences, mask=mask)
                
                # Fix for duplicates in last batch
                if self.accelerator is not None:
                    self.accelerator.wait_for_everyone()
                    embeddings = self.accelerator.gather_for_metrics((embedding))
                    if self.accelerator.is_main_process:
                        dataset_embeds.append(embeddings.detach().cpu().numpy())
                else:
                    dataset_embeds.append(embedding.detach().cpu().numpy())
        embeddings = np.vstack(dataset_embeds)
        return embeddings

    def fine_tune(
            self,
            fine_tune_head: torch.nn.Module,
            train_input_data: UCEDataset, 
            train_labels,     
            validation_input_data = None,
            validation_labels = None,
            optimizer: optim = optim.AdamW,
            optimizer_params: dict = {'lr': 0.0001}, 
            loss_function: loss = loss.CrossEntropyLoss(), 
            epochs: int = 1,
            freeze_layers: int = 0,
            lr_scheduler_params: Optional[dict] = None) -> UCEFineTuningModel:
        """Fine-tunes the scGPT model with different head modules. 

        Parameters
        ----------
        fine_tune_head : torch.nn.Module
            The head module to be used for fine-tuning. This should be a torch.nn.Module.
        train_input_data : Dataset
            A helical scGPT processed dataset for fine-tuning
        train_labels : ndarray
            The labels for the training data. These should be stored as unique per class integers.
        validation_input_data : Dataset, default = None
            A helical scGPT processed dataset for per epoch validation. If this is not specified, no validation will be performed.
        validation_labels : ndarray, default = None,
            The labels for the validation data. These should be stored as unique per class integers.
        optimizer : torch.optim, default = torch.optim.AdamW
            The optimizer to be used for training.
        optimizer_params : dict
            The optimizer parameters to be used for the optimizer specified. This list should NOT include model parameters.
            e.g. optimizer_params = {'lr': 0.0001}
        loss_function : torch.nn.modules.loss, default = torch.nn.modules.loss.CrossEntropyLoss()
            The loss function to be used.
        epochs : int, optional, default = 10
            The number of epochs to train the model
        freeze_layers : int, optional, default = 0
            The number of layers to freeze.
        lr_scheduler_params : dict, default = None
            The learning rate scheduler parameters for the transformers get_scheduler method. The optimizer will be taken from the optimizer input and should not be included in the learning scheduler parameters. If not specified, no scheduler will be used.
            e.g. lr_scheduler_params = { 'name': 'linear', 'num_warmup_steps': 0, 'num_training_steps': 5 }

        Returns
        -------
        torch.nn.Module
            The fine-tuned model.
        """
        batch_size = self.config["batch_size"]
        dataloader = DataLoader(train_input_data, 
                                batch_size=batch_size, 
                                shuffle=False,
                                collate_fn=train_input_data.collator_fn,
                                num_workers=0)
        
        if validation_input_data is not None:
            validation_dataloader = DataLoader(validation_input_data, 
                        batch_size=batch_size, 
                        shuffle=False,
                        collate_fn=validation_input_data.collator_fn,
                        num_workers=0)

        if self.accelerator is not None:
            dataloader = self.accelerator.prepare(dataloader)
            if validation_input_data is not None:
                validation_dataloader = self.accelerator.prepare(validation_dataloader)


        # disable progress bar if not the main process
        # if self.accelerator is not None:
        #     pbar = tqdm(dataloader, disable=not self.accelerator.is_local_main_process)
        # else:
        #     pbar = tqdm(dataloader)

        model = UCEFineTuningModel(self, fine_tune_head).to(self.device)
        
        model.train()

        optimizer = optimizer(model.parameters(), **optimizer_params)

        lr_scheduler = None
        if lr_scheduler_params is not None: 
            lr_scheduler = get_scheduler(optimizer=optimizer, **lr_scheduler_params)

        for j in range(epochs):
            batch_count = 0
            batch_loss = 0.0
            batches_processed = 0
            training_loop = tqdm(dataloader, desc="Fine-Tuning")
            for batch in training_loop:
                batch_sentences, mask, idxs = batch[0], batch[1], batch[2]
                batch_sentences = batch_sentences.permute(1, 0)
                if self.config["multi_gpu"]:
                    batch_sentences = self.model.module.pe_embedding(batch_sentences.long())
                else:
                    batch_sentences = self.model.pe_embedding(batch_sentences.long())
                batch_sentences = torch.nn.functional.normalize(batch_sentences, dim=2)  # normalize token outputs
                output = model.forward(batch_sentences, mask=mask)
                labels = torch.tensor(train_labels[batch_count: batch_count + self.config["batch_size"]], device=self.device)
                batch_count += self.config["batch_size"]
                loss = loss_function(output, labels)
                loss.backward()
                batch_loss += loss.item()
                batches_processed += 1

                optimizer.step()
                optimizer.zero_grad()

                training_loop.set_postfix({"loss": batch_loss/batches_processed})
                training_loop.set_description(f"Fine-Tuning: epoch {j+1}/{epochs}")

            if lr_scheduler is not None:
                lr_scheduler.step()

            if validation_input_data is not None:
                testing_loop = tqdm(validation_dataloader, desc="Fine-Tuning Validation")
                accuracy = 0.0
                count = 0.0
                validation_batch_count = 0
                for validation_data in testing_loop:
                    batch_sentences, mask, idxs = validation_data[0], validation_data[1], validation_data[2]
                    batch_sentences = batch_sentences.permute(1, 0)
                    if self.config["multi_gpu"]:
                        batch_sentences = self.model.module.pe_embedding(batch_sentences.long())
                    else:
                        batch_sentences = self.model.pe_embedding(batch_sentences.long())
                    batch_sentences = torch.nn.functional.normalize(batch_sentences, dim=2)  # normalize token outputs
                    output = model.forward(batch_sentences, mask=mask)
                    val_labels = torch.tensor(validation_labels[validation_batch_count: validation_batch_count + self.config["batch_size"]], device=self.device)
                    validation_batch_count += self.config["batch_size"]
                    accuracy += accuracy_score(val_labels.cpu(), torch.argmax(output, dim=1).cpu())
                    count += 1.0
                    testing_loop.set_postfix({"accuracy": accuracy/count})
    
                
        return None